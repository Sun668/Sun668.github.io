<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="apple-touch-icon" sizes="180x180" href="/en/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/en/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/en/images/favicon-16x16-next.png"><link rel="mask-icon" href="/en/images/logo.svg" color="#222"><meta name="google-site-verification" content="7yRqtT6cCpC-_R-rzM9gUoQGmheV9OZAUKIXrbAsyqE"><link rel="stylesheet" href="/en/css/main.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><script class="next-config" data-name="main" type="application/json">{"hostname":"sunra.top","root":"/en/","images":"/en/images","scheme":"Muse","darkmode":false,"version":"8.17.1","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/en/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/en/js/config.js"></script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8623125811074939" crossorigin="anonymous"></script><script>!function(e,p,s,r){if(void 0===e.webpushr){e.webpushr=e.webpushr||function(){(e.webpushr.q=e.webpushr.q||[]).push(arguments)};var d,t=p.getElementsByTagName(s)[0];(d=p.createElement(s)).id="webpushr-jssdk",d.async=1,d.src="https://cdn.webpushr.com/app.min.js",t.parentNode.appendChild(d)}}(window,document,"script"),webpushr("setup",{key:"BEQjc-0d1Q1CubrYZ2e2XXv5Is0ZCd3CtrNLet06owkUWK68qkxHpho2mKdnj2vpGdxddRDxRYthLuMwrTqfSB4"})</script><meta name="description" content="General descriptionThis article roughly summarizes how some 3D space coordinate data is transformed into pixels of different colors on the screen through the rendering pipeline. When reading, you need"><meta property="og:type" content="article"><meta property="og:title" content="Unity Rendering Principle (1) Rendering Pipeline - From Point on Model to Point on Screen"><meta property="og:url" content="https://sunra.top/en/posts/63218/index.html"><meta property="og:site_name" content="Origin of Ray"><meta property="og:description" content="General descriptionThis article roughly summarizes how some 3D space coordinate data is transformed into pixels of different colors on the screen through the rendering pipeline. When reading, you need"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646000/origin-of-ray/render_pipeline_gnuamz.png"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646124/origin-of-ray/asynccode_q28ocq.png"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646148/origin-of-ray/asynccode_ginlkt.png"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646175/origin-of-ray/asynccode_kopbkx.png"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646237/origin-of-ray/asynccode_kgktgg.png"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646263/origin-of-ray/asynccode_s4tmq5.png"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646294/origin-of-ray/asynccode_re54bh.png"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646323/origin-of-ray/asynccode_b7gtzl.png"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646357/origin-of-ray/asynccode_viggia.png"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646391/origin-of-ray/asynccode_wyxw6o.png"><meta property="og:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646424/origin-of-ray/asynccode_dk5lwg.png"><meta property="article:published_time" content="2021-12-16T09:21:28.000Z"><meta property="article:modified_time" content="2023-07-03T14:10:44.103Z"><meta property="article:author" content="Ray Sun"><meta property="article:tag" content="Technology Sharing Using Tutorials Principles Front End Computer Graphics"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646000/origin-of-ray/render_pipeline_gnuamz.png"><link rel="canonical" href="https://sunra.top/en/posts/63218/"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://sunra.top/en/posts/63218/","path":"posts/63218/","title":"Unity Rendering Principle (1) Rendering Pipeline - From Point on Model to Point on Screen"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>Unity Rendering Principle (1) Rendering Pipeline - From Point on Model to Point on Screen | Origin of Ray</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-KEJ1L66CKC"></script><script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-KEJ1L66CKC","only_pageview":false}</script><script src="/en/js/third-party/analytics/google-analytics.js"></script><script src="/en/js/third-party/analytics/baidu-analytics.js"></script><script async src="https://hm.baidu.com/hm.js?cc2e15dfd66849cf1d7843d0d532438e"></script><link rel="dns-prefetch" href="https://blog-comments-3w44.vercel.app/"><noscript><link rel="stylesheet" href="/en/css/noscript.css"></noscript><link rel="alternate" href="/en/atom.xml" title="Origin of Ray" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar" role="button"><span class="toggle-line"></span><span class="toggle-line"></span><span class="toggle-line"></span></div></div><div class="site-meta"><a href="/en/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">Origin of Ray</p><i class="logo-line"></i></a><p class="site-subtitle" itemprop="description">Lift the fog of the Internet together</p></div><div class="site-nav-right"><div class="toggle popup-trigger" aria-label="Search" role="button"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/en/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-categories"><a href="/en/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/en/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-english"><a href="https://sunra.top/en" rel="section"><i class="fa fa-language fa-fw"></i>English</a></li><li class="menu-item menu-item-中文"><a href="https://sunra.top/" rel="section"><i class="fa fa-language fa-fw"></i>中文</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc"> Table of Contents</li><li class="sidebar-nav-overview"> Overview</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#General-description"><span class="nav-number">1.</span> <span class="nav-text">General description</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What-is-a-rendering-pipeline"><span class="nav-number">2.</span> <span class="nav-text">What is a rendering pipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-task-of-rendering-pipeline"><span class="nav-number">2.1.</span> <span class="nav-text">The task of rendering pipeline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Three-conceptual-stages-of-the-rendering-pipeline"><span class="nav-number">2.2.</span> <span class="nav-text">Three conceptual stages of the rendering pipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Application-phase"><span class="nav-number">2.2.1.</span> <span class="nav-text">Application phase</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Geometric-stage"><span class="nav-number">2.2.2.</span> <span class="nav-text">Geometric stage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Grating-stage"><span class="nav-number">2.2.3.</span> <span class="nav-text">Grating stage</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Communication-between-CPU-and-GPU"><span class="nav-number">3.</span> <span class="nav-text">Communication between CPU and GPU</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Load-data-into-video-memory"><span class="nav-number">3.1.</span> <span class="nav-text">Load data into video memory</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Set-render-state"><span class="nav-number">3.2.</span> <span class="nav-text">Set render state</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Call-Draw"><span class="nav-number">3.3.</span> <span class="nav-text">Call Draw</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GPU-pipeline"><span class="nav-number">4.</span> <span class="nav-text">GPU pipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Vertex-shader"><span class="nav-number">4.1.</span> <span class="nav-text">Vertex shader</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cropping"><span class="nav-number">4.2.</span> <span class="nav-text">Cropping</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Screen-mapping"><span class="nav-number">4.3.</span> <span class="nav-text">Screen mapping</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Triangle-settings"><span class="nav-number">4.4.</span> <span class="nav-text">Triangle settings</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Triangular-traversal"><span class="nav-number">4.5.</span> <span class="nav-text">Triangular traversal</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Element-shader"><span class="nav-number">4.6.</span> <span class="nav-text">Element shader</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Piece-by-slice-operation"><span class="nav-number">4.7.</span> <span class="nav-text">Piece-by-slice operation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Template-testing"><span class="nav-number">4.7.1.</span> <span class="nav-text">Template testing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Depth-test"><span class="nav-number">4.7.2.</span> <span class="nav-text">Depth test</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Merge"><span class="nav-number">4.7.3.</span> <span class="nav-text">Merge</span></a></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="Ray Sun" src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1592617514/avatar_rpap6c.jpg"><p class="site-author-name" itemprop="name">Ray Sun</p><div class="site-description" itemprop="description">Lift the fog of the Internet together</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/en/archives/"><span class="site-state-item-count">268</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"> <a href="/en/categories/"><span class="site-state-item-count">17</span> <span class="site-state-item-name">categories</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/Sun668" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Sun668" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:947692259@qq.com" title="E-Mail → mailto:947692259@qq.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i> E-Mail</a></span></div><div class="cc-license animated" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="external nofollow noopener noreferrer" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a></div></div></div></div><div class="wechat_channel" style="width:50%;margin-left:25%"><br> <img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1685836114/origin-of-ray/wechat_channel_zmg0hw.jpg"></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en"><link itemprop="mainEntityOfPage" href="https://sunra.top/en/posts/63218/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1592617514/avatar_rpap6c.jpg"><meta itemprop="name" content="Ray Sun"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Origin of Ray"><meta itemprop="description" content="Lift the fog of the Internet together"></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="Unity Rendering Principle (1) Rendering Pipeline - From Point on Model to Point on Screen | Origin of Ray"><meta itemprop="description" content></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> Unity Rendering Principle (1) Rendering Pipeline - From Point on Model to Point on Screen</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">Posted on</span> <time title="Created: 2021-12-16 17:21:28" itemprop="dateCreated datePublished" datetime="2021-12-16T17:21:28+08:00">2021-12-16</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i></span> <span class="post-meta-item-text">Edited on</span> <time title="Modified: 2023-07-03 22:10:44" itemprop="dateModified" datetime="2023-07-03T22:10:44+08:00">2023-07-03</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i></span> <span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/en/categories/Unity/" itemprop="url" rel="index"><span itemprop="name">Unity</span></a></span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i></span> <span class="post-meta-item-text">Waline:</span><a title="waline" href="/en/posts/63218/#waline" itemprop="discussionUrl"><span class="post-comments-count waline-comment-count" data-path="/en/posts/63218/" itemprop="commentCount"></span></a></span></div></div></header><div class="post-body" itemprop="articleBody"><h1 id="General-description"><a href="#General-description" class="headerlink" title="General description"></a>General description</h1><p>This article roughly summarizes how some 3D space coordinate data is transformed into pixels of different colors on the screen through the rendering pipeline. When reading, you need to pay attention to the difference between data elements, slice elements, and pixels.</p><p>The following figure is an overall flow chart, and the specific details can be seen later.</p><span id="more"></span><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646000/origin-of-ray/render_pipeline_gnuamz.png" alt></p><h1 id="What-is-a-rendering-pipeline"><a href="#What-is-a-rendering-pipeline" class="headerlink" title="What is a rendering pipeline"></a>What is a rendering pipeline</h1><h2 id="The-task-of-rendering-pipeline"><a href="#The-task-of-rendering-pipeline" class="headerlink" title="The task of rendering pipeline"></a>The task of rendering pipeline</h2><p>The job of the rendering pipeline is to render a two-dimensional image from a three-dimensional scene.</p><p>That is to say, it is necessary to start from a series of three-dimensional spatial information, such as vertex data, texture and other information, and finally convert this information into a two-dimensional image that the human eye can see.</p><p>This work is usually done jointly by the CPU and GPU.</p><h2 id="Three-conceptual-stages-of-the-rendering-pipeline"><a href="#Three-conceptual-stages-of-the-rendering-pipeline" class="headerlink" title="Three conceptual stages of the rendering pipeline"></a>Three conceptual stages of the rendering pipeline</h2><p>Note that these three stages are only conceptual stages, and there are many steps inside each stage, usually a pipeline system.</p><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646124/origin-of-ray/asynccode_q28ocq.png" alt="img"></p><h3 id="Application-phase"><a href="#Application-phase" class="headerlink" title="Application phase"></a>Application phase</h3><p>The reason why it is called the application phase is because this phase is the work carried out in our application, usually executed in the CPU, and the developer has absolute control over this phase.</p><p>At this stage, developers have three main tasks:</p><ol><li><p>Prepare scene data, such as the position of the camera, the viewing cone, the point information of the model in the scene, the position intensity of the light source and other information.</p></li><li><p>With the above information, in order to improve rendering performance, we need to do a culling of Coarse Grain to remove those invisible objects, so that there is no need to hand them over to the geometry stage.</p></li></ol><p>Note that culling is different from the later clipping. It completely eliminates the object information that is not in the visual cone, and directly discards the object information. The clipping mentioned later refers to those that are half in the visual cone and half are not. For this kind, while discarding part of the point information, we also add the intersection information when the object and the visual cone intersect.</p><ol><li>Finally, we need to set the rendering state of each model, including but not limited to materials (diffuse color, highlight color), textures, shaders, etc. The most important thing at this stage is to output the geometric information required for rendering, that is, render graph primitives. In general, graph render elements can be points, lines, triangles, etc.</li></ol><p>The result of the application phase is all the 3D information that needs to be rendered.</p><h3 id="Geometric-stage"><a href="#Geometric-stage" class="headerlink" title="Geometric stage"></a>Geometric stage</h3><p>The Geometry phase deals with all things related to the geometry we want to draw. For example, deciding what elements to draw, how to draw them, and where to draw them, this phase is usually done on the GPU</p><p>The geometry phase is responsible for dealing with each render graph element and then performing vertex-by-vertex, polygon-by-polygon operations. This phase can be further divided into smaller pipeline stages.</p><p>An important task in the geometry stage is to transform the vertex coordinates into screen coordinates, and then hand them over to the grater for processing.</p><p>After multi-step processing of the input primitive, this stage will output the two-dimensional vertex coordinates of the screen space, the depth value corresponding to each vertex, coloring and other related information, and pass it to the next stage.</p><h3 id="Grating-stage"><a href="#Grating-stage" class="headerlink" title="Grating stage"></a>Grating stage</h3><p>This stage will use the data passed in the previous stage to generate pixels on the screen and render the final image.</p><p>This stage also runs on the GPU.</p><p>The main task of grating is to decide which pixels of each render graph element should be drawn on the screen. It needs to interpolate the vertex-by-vertex data (such as texture coordinates, vertex colors) obtained in the previous stage, and then perform pixel-by-pixel processing.</p><p>The point of the result obtained in the previous step is not one-to-one correspondence with the pixels on the screen. For example, suppose that the coordinates of the pixels on the screen are all integers, and the screen coordinates calculated in the previous step are likely to be decimal places. We cannot simply and roughly choose the value of this point to be given to the nearest pixel, but to perform interpolation calculations.</p><h1 id="Communication-between-CPU-and-GPU"><a href="#Communication-between-CPU-and-GPU" class="headerlink" title="Communication between CPU and GPU"></a>Communication between CPU and GPU</h1><p>The starting point of the rendering pipeline is the CPU, which is the application phase. The application phase can be roughly divided into three stages:</p><ol><li><p>Load the data into video memory.</p></li><li><p>Set render status</p></li><li><p>Call Draw Call to notify the GPU.</p></li></ol><h2 id="Load-data-into-video-memory"><a href="#Load-data-into-video-memory" class="headerlink" title="Load data into video memory"></a>Load data into video memory</h2><p>All the data needed for rendering needs to be loaded from the hard disk into system memory. Then data such as meshes and textures are loaded into the storage space on the graphics card - video memory. This is because graphics cards have faster access to video memory, while most graphics cards do not have direct access rights to memory.</p><p>When the data is loaded into video memory, the data in RAM can be removed. But for some data, the CPU still needs to access it (for example, we want the CPU to have access to grid data to detect collisions), so we may not want this data to be removed.</p><h2 id="Set-render-state"><a href="#Set-render-state" class="headerlink" title="Set render state"></a>Set render state</h2><p>A popular explanation is that these states define how the mesh in the scene is rendered, such as which vertex shader, slice shader, light source properties, materials, etc.</p><p>If we don’t change the render state, then all meshes will use the same render state.</p><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646148/origin-of-ray/asynccode_ginlkt.png" alt="img"></p><h2 id="Call-Draw"><a href="#Call-Draw" class="headerlink" title="Call Draw"></a>Call Draw</h2><p>After all the above work is ready, the CPU needs to call a rendering command to tell the GPU to work according to the set rendering data and rendering state.</p><p>When receiving a Draw call, the GPU will calculate based on the rendering state (such as materials, textures, shaders, etc.) and all input vertex data, and finally output them as pixels displayed on the screen.</p><h1 id="GPU-pipeline"><a href="#GPU-pipeline" class="headerlink" title="GPU pipeline"></a>GPU pipeline</h1><p>After the application phase, the CPU notifies the GPU through the Draw call command to render according to the data generated by the CPU, and then enters the GPU pipeline.</p><p>For the last two stages of the concept stage, namely the geometry stage and the rasterization stage, the developer cannot have absolute control, and the carrier of its implementation is the GPU. The GPU greatly speeds up the rendering speed by implementing pipelining.</p><p>Although we cannot fully control the implementation details of these two stages, the GPU opens up a lot of control to developers</p><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646175/origin-of-ray/asynccode_kopbkx.png" alt="img"></p><p>As can be seen from the figure, the GPU’s rendering pipeline receives vertex data as input. These vertex data are loaded into video memory by the application phase and specified by the Draw call, and these data are then passed to the vertex shader.</p><ul><li><p>Vertex shader. It is fully programmable and is usually used to implement functions such as spatial changes of vertices, vertex shading, etc.</p></li><li><p>Surface Subdivision Shader. Is an optional shader that is used to subdivide data.</p></li><li><p>Geometry shader. It is an optional shader used to perform element-by-element shading operations or generate more elements.</p></li><li><p>Crop. The purpose of this stage is to crop out the vertices that are not in the camera’s field of view and remove some triangular elements. This stage is configurable, for example, we can use a custom crop plane to configure the crop area, or we can control the front or back of the triangle element through commands.</p></li><li><p>Screen mapping. Not configurable and programmable, responsible for translating the coordinates of each primitive into the screen coordinate system.</p></li><li><p>Triangle setting and triangle traversal. Fixed function phase</p></li><li><p>Element shader. Element-by-element shading operation.</p></li><li><p>Piece-by-slice meta operation. Responsible for performing many important operations such as modifying colors, depth buffering, blending, etc., non-programmable, but highly configurable.</p></li></ul><h2 id="Vertex-shader"><a href="#Vertex-shader" class="headerlink" title="Vertex shader"></a>Vertex shader</h2><p>In the first stage of the pipeline, the input comes from the CPU, and the processing unit of the vertex shader is the vertex.</p><p>That is, each vertex entered will call the vertex shader once.</p><p>The vertex shader itself cannot create or destroy any vertices, and cannot obtain the relationship between vertices. For example, we cannot know whether two vertices belong to the same triangular network. But precisely because of this independence, GPU can use its own characteristics to parallelize each vertex without being blocked by other vertices.</p><p>The main functions of the vertex shader are coordinate changes and vertex-by-vertex lighting.</p><p>Of course, in addition to these two main tasks, vertex shaders can also output the data required for subsequent stages</p><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646237/origin-of-ray/asynccode_kgktgg.png" alt="img"></p><p>Coordinate transformation. As the name suggests, it is to perform some kind of transformation on the coordinates of the vertices. Vertex shaders can change the position of the vertices in this step, which is very useful in vertex animation. For example, we can simulate water surface, cloth, etc. by changing the vertex position. But it should be noted that no matter how we change the vertices in the vertex shader, one of the tasks a vertex shader must do is: convert the vertex coordinates from model space to<a target="_blank" rel="external nofollow noopener noreferrer" href="https://gameinstitute.qq.com/community/detail/117556">齐次裁剪空间</a></p><p>We can often see the following code in vertex shaders:</p><p>o.pos = mul(UNITY_MVP, v.position);</p><p>The purpose of this code is to convert the vertex coordinates to the homogeneous clipping coordinate system, and then usually do the perspective division by the hardware, and finally get the normalized device coordinates (Normalized Device Coordinate, NDC).</p><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646263/origin-of-ray/asynccode_s4tmq5.png" alt="img"></p><p>Note that the coordinate range given in the figure above is both OpenGL and Unity’s NDC, and its z component has a range between [-1, 1], while in DirectX, the z component of the NDC has a range of [0, 1]. Vertex shaders can have different output methods.</p><p>The most common output path is grated and then handed over to the chip element shader for processing. In modern Shader Models, data can be sent to the Surface Subdivision Shader or Geometry Shader.</p><h2 id="Cropping"><a href="#Cropping" class="headerlink" title="Cropping"></a>Cropping</h2><p>Since our scene is very large, and the camera’s field of view is likely not to cover all scene objects, it is a natural idea that those objects that are not in the field of view do not need to be processed, and Clipping is for this.</p><p>There are three relationships between a primitive and the camera field of view: completely within the field of view, partially within the field of view, and completely outside the field of view.</p><p>What is completely within the field of view is passed to the next pipeline, and what is completely outside the field of view does not need to be passed.</p><p>The part that is in the field of view needs to be cropped.</p><p>Since we have normalized all the vertices into a cube in the previous step, clipping is simple: just crop the primitive into the unit cube:</p><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646294/origin-of-ray/asynccode_re54bh.png" alt="img"></p><p>Unlike vertex shaders, this step is not programmable, that is, we cannot control the clipping process through programming.</p><h2 id="Screen-mapping"><a href="#Screen-mapping" class="headerlink" title="Screen mapping"></a>Screen mapping</h2><p>The input coordinates in this step are still the coordinates in the 3D coordinate system (the range is within the unit cube). The task of screen mapping is to convert the x and y of each primitive to the screen coordinate system (two-dimensional coordinate system), which has a lot to do with our display screen resolution</p><p>Suppose that we need to render the scene onto a window, the window range is from the smallest window coordinates (x1, y1), to the largest window coordinates (x2, y2), since our input coordinates are -1 to 1, From this you can imagine that this process is actually a scaling process, in which the z coordinate remains unchanged.</p><p>The screen map does not do anything with the z coordinate, and the screen coordinate system and the z coordinate together form a new coordinate system, called the window coordinate system, and these values are passed together to the grating stage</p><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646323/origin-of-ray/asynccode_b7gtzl.png" alt="img"></p><h2 id="Triangle-settings"><a href="#Triangle-settings" class="headerlink" title="Triangle settings"></a>Triangle settings</h2><p>From this step we enter the grating phase.</p><p>The information output from the previous stage is the position of the vertices in the screen coordinate system and additional information related to them, such as depth value (z coordinate), normal direction, viewing angle direction, etc.</p><p>The two most important goals of grating are to calculate which pixels are covered by each primitive, and to calculate their colors for those pixels.</p><p>The first pipeline stage of grating is triangle setup. This stage calculates the information needed to grate a triangle mesh.</p><p>Specifically, the output from the previous stage is the vertices of the triangular mesh, that is, we get two vertices on each edge of the triangular mesh. But if we want to get the coverage of the entire triangular mesh on pixels, we must calculate the pixel coordinates on each edge.</p><p>In order to be able to calculate the coordinate information of the boundary pixels, we need to obtain a representation of the boundary of the triangle.</p><p>Such a process of calculating the data represented by a triangular mesh is called triangle setup. Its output prepares for the next stage.</p><h2 id="Triangular-traversal"><a href="#Triangular-traversal" class="headerlink" title="Triangular traversal"></a>Triangular traversal</h2><p>Each pixel (pixel on the screen) is checked if it is covered by a triangular mesh, and if it is covered, a slice is generated. Such a process of finding which pixels are covered by the triangular mesh is triangle traversal, which is also known as scan transformation.</p><p>The triangle traversal stage will determine which pixels are covered by a triangular mesh based on the results of the previous stage, and use the three vertex information of the triangular mesh to interpolate the pixels of the entire coverage area.</p><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646357/origin-of-ray/asynccode_viggia.png" alt="img"></p><p>The result of this step is a sequence of elements.</p><p>A slice corresponds to a pixel, but it is not a pixel in the true sense, but a collection of states that are used to calculate the final color of each pixel. These states include but are not limited to its screen coordinates, depth information, and other vertex information output from the geometry phase, such as normals, texture coordinates, etc.</p><h2 id="Element-shader"><a href="#Element-shader" class="headerlink" title="Element shader"></a>Element shader</h2><p>A very important stage of programmable shaders, also known as pixel shaders, but chip shaders are more appropriate because chip shaders are not yet a true pixel.</p><p>The previous grating stage does not affect the pixels on the screen, but generates a series of data to describe how a triangular grid covers each pixel. And each chip element stores such a series of data.</p><p>What really affects the pixels is the next stage of the pipeline - slice-by-slice operations.</p><p>The input of the slice element shader is the result of the difference value of the vertex information obtained in the previous stage.</p><p>This stage can complete a lot of important rendering techniques, one of the most important techniques is texture sampling, in order to sample the texture element shader, we usually output the texture coordinates of each vertex in the vertex shader, and then through the grating phase of the three vertices of the triangular mesh corresponding to the texture coordinates of the interpolation, you can get the texture coordinates of the chip element covered.</p><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646391/origin-of-ray/asynccode_wyxw6o.png" alt="img"></p><h2 id="Piece-by-slice-operation"><a href="#Piece-by-slice-operation" class="headerlink" title="Piece-by-slice operation"></a>Piece-by-slice operation</h2><p>This step is the final step in the rendering pipeline, also known as the output merge phase.</p><p>There are several main tasks in this stage:</p><ul><li><p>Determine the visibility of each element, which involves a lot of testing work, such as in-depth testing, template testing, etc.</p></li><li><p>If a chip passes all tests, it is necessary to merge the color value of the chip with the color already stored in the color buffer, that is, blend.</p></li></ul><p>Piece-by-piece operation first needs to solve the visibility of each piece element. This requires a series of tests. If the test cannot be passed, the piece element will be discarded directly, and the previous work will be in vain.</p><p>Briefly introduce template testing and in-depth testing</p><p><img src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1639646424/origin-of-ray/asynccode_dk5lwg.png" alt="img"></p><h3 id="Template-testing"><a href="#Template-testing" class="headerlink" title="Template testing"></a>Template testing</h3><p>Related to this is template buffering, which, like color buffering or depth buffering, can be understood as something like a register in an operating system for temporary storage.</p><p>When template testing is enabled, the GPU will first read the template value of the chip element location in the template buffer (through a mask, similar to a subnet mask), and then compare the value with the read reference value.</p><p>This comparison function can be specified by the developer, such as discard the element when less than, or discard when greater than</p><h3 id="Depth-test"><a href="#Depth-test" class="headerlink" title="Depth test"></a>Depth test</h3><p>If the depth test is turned on, the GPU will compare the depth value of the film element with the depth value of the depth buffer. This comparison is the same as the template test and can also be specified by the developer. Usually, this comparison function is less than or equal to the relationship, because we In general, we only want to display points close to the camera.</p><p>After passing the template test, the developer can also specify whether to update the template buffer content with this value.</p><h3 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h3><p>Our rendering is object by object drawn onto the screen, and the color value of each pixel is stored in the color buffer.</p><p>When we perform this render, we often have the last result in the color buffer, so whether we overwrite directly or blend in some way is the problem solved by merging.</p><p>For opaque objects, just turn off the blending and cover it.</p><p>But for translucent objects, we need to combine the contents of the buffer to mix.</p></div><footer class="post-footer"><div class="post-copyright"><ul><li class="post-copyright-author"> <strong>Post author:</strong> Ray Sun</li><li class="post-copyright-link"> <strong>Post link:</strong> <a href="https://sunra.top/en/posts/63218/" title="Unity Rendering Principle (1) Rendering Pipeline - From Point on Model to Point on Screen">https://sunra.top/en/posts/63218/</a></li><li class="post-copyright-license"> <strong>Copyright Notice:</strong> All articles in this blog are licensed under<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i> BY-NC-SA</a> unless stating additionally.</li></ul></div><div class="followme"> <span>Welcome to my other publishing channels</span><div class="social-list"><div class="social-item"><span class="social-link"><span class="icon"><i class="fab fa-weixin"></i></span> <span class="label">WeChat</span></span> <img class="social-item-img" src="https://res.cloudinary.com/dvtfhjxi4/image/upload/v1685836114/origin-of-ray/wechat_channel_zmg0hw.jpg"></div><div class="social-item"><a target="_blank" class="social-link" href="/en/atom.xml"><span class="icon"><i class="fa fa-rss"></i></span> <span class="label">RSS</span></a></div></div></div><div class="post-nav"><div class="post-nav-item"><a href="/en/posts/33455/" rel="prev" title="Unity Performance optimization summary"><i class="fa fa-chevron-left"></i> Unity Performance optimization summary</a></div><div class="post-nav-item"> <a href="/en/posts/28755/" rel="next" title="Operating System Learning Notes (5) Fundamentals of File Management System">Operating System Learning Notes (5) Fundamentals of File Management System<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="waline"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Ray Sun</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="external nofollow noopener noreferrer" target="_blank">NexT.Muse</a></div></div></footer><div class="toggle sidebar-toggle" role="button"><span class="toggle-line"></span><span class="toggle-line"></span><span class="toggle-line"></span></div><div class="sidebar-dimmer"></div><div class="back-to-top" role="button" aria-label="Back to top"><i class="fa fa-arrow-up fa-lg"></i> <span>0%</span></div><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="/en/js/comments.js"></script><script src="/en/js/utils.js"></script><script src="/en/js/motion.js"></script><script src="/en/js/schemes/muse.js"></script><script src="/en/js/next-boot.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script><script src="/en/js/third-party/search/local-search.js"></script><script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script><script src="/en/js/third-party/math/mathjax.js"></script><script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"https://blog-comments-3w44.vercel.app/","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"placeholder":"欢迎大家交流学习","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"visitor":true,"comment_count":true,"requiredFields":[],"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","el":"#waline","comment":true,"path":"/en/posts/63218/"}</script><link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css"><script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script></body></html>